model_config:
  lags: 512
  embedding_dim: 768
  n_blocks: 12
  pool_size: 4
  number_of_heads: 4
  number_ts: 264
  number_of_clusters: None
  ## As long as really want to experiment, do not touch any part of below ##
  ## Upsampling_Details ##
  conv_activation: F.gelu
  conv_FFN_activation: F.gelu
  channel_shuffle: True
  channel_shuffle_group: 2
  conv_dropout_FFN: 0.2
  conv_dropout_linear: 0.2
  conv_FFN_bias: True
  conv_FFN_expansion_size: 2
  conv_bias: True
  # Attention_Block_Details #
  attenttion_FFN_dropout: 0.2
  attenttion_FFN_activation: F.gelu
  attenttion_FFN_bias: True
  attenttion_FFN_expansion_size: 4
  ## check the attention dropout from pytorchs attention
  

      
trainer_config:
  save_every: 1
  max_epochs: 5
  snapshot_name: "small_model"
  snapshot_dir: "/model"
  compile_model: True
  use_wnb: False

  
optimizer_scheduler_config:
  optimizer_config:
    lr: 0.0001875  #0.0005*batch_size*GPU/512
    weight_decay: 0.05
    momentum: 0.9

  scheduler_config:
    cosine:
      T_0: 300
      eta_min: 0.0000088
    linear_scheduler:
      total_iters: 5
      start_factor: 0.0002
  ###Scheduler and optimizer config should be given to the cfg file.

data:
  train_path:
   file: "data/array_train.dat"
   length_file: "data/lengthsarray_train.dat"
   file_names: "data/names_array_train.txt"
   lags: 512 # 512 -> 1 prediction, we will do
  val_path:
   file: "data/array_test.dat"
   length_file: "data/lengthsarray_test.dat"
   file_names: "data/names_array_test.txt"
   lags: 512 # 512 -> 1 prediction, we will do

  
  train_data_details:
   batch_size: 256
   num_workers: 4
 #  shuffle: True
   pin_memory: True
   persistent_workers: True
   prefetch_factor: 2

  val_data_details:
   batch_size: 256
   num_workers: 4
#   shuffle: False
   pin_memory: True
   drop_last: True
   

