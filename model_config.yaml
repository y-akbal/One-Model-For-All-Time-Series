model_config:
  lags: 512
  embedding_dim: 768
  n_blocks: 12
  pool_size: 4
  number_of_heads: 8
  number_ts: 264
  #number_of_clusters: None
  ## As long as really want to experiment, do not touch any part of below ##
  ## Upsampling_Details ##
  #conv_activation: F.gelu
  #conv_FFN_activation: F.gelu
  channel_shuffle: True
  channel_shuffle_group: 2
  conv_dropout_FFN: 0.2
  conv_dropout_linear: 0.2
  conv_FFN_bias: True
  conv_FFN_expansion_size: 2
  conv_bias: True
  # Attention_Block_Details #
  attention_FFN_dropout: 0.2
  #attention_FFN_activation: F.gelu
  attention_FFN_bias: True
  attention_FFN_expansion_size: 4
  
  
### NOTES: 
###1) DO A FINAL CHECK ON ATTENTION DROPOUT STUFF
### 1.25) Adapth your code so that single GPU training works properly!!! with no reference do DDP sort'o stuff
### 1.35) Watch out the model shapes for torch.compile
###1.5) Loss syncronization along the worker remains problematic --> this is fixed, need to do sync loss after each epoch!!!
###2) ATTENTION MASK IS IMPORTANT THEREFORE SHOULD HAVE DYNAMIC SHAPE
###3) CHECK LOSS LOGGER AND VAL_LOSS LOGGER AND WANDB STUFF IS IMPORTANT!!!
###4) CHECK THE AVERAGE AND STD OF OUTPUT AT 0TH INITIALIZATION
###5) DO SYSTEMATIC ANALYSIS ON R2 VALUES SOMEWHAT
###6) SOME PROFILING WOULD BE VERY GOOD!!!
####7) dataset_generator return entire X not X[:-1].... ## DONE
###8) Do experiments with activation in the convolution
      
trainer_config:
  save_every: 1
  max_epochs: 5
  snapshot_name: "small_model"
  snapshot_dir: "model"
  compile_model: False
  use_wnb: False

  
optimizer_config:
  lr: 0.0001
  weight_decay: 0.09
  #momentum: 0.9

scheduler_config:
  T_0: 300
  eta_min: 0.0000088


data:
  train_path:
   file: "data/array_train.dat"
   length_file: "data/lengthsarray_train.dat"
   file_names: "data/names_array_train.txt"
   lags: 512 # 512 -> 1 prediction, we will do
  val_path:
   file: "data/array_test.dat"
   length_file: "data/lengthsarray_test.dat"
   file_names: "data/names_array_test.txt"
   lags: 512 # 512 -> 1 prediction, we will do

  
  train_data_details:
   batch_size: 64
   num_workers: 6
 # shuffle: True
   pin_memory: True
   persistent_workers: True
   prefetch_factor: 2

  val_data_details:
   batch_size: 128
   num_workers: 4
#   shuffle: False
   pin_memory: True
   drop_last: True
   

