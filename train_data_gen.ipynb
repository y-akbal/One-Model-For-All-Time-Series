{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f675f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import functional as F\n",
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import concurrent\n",
    "list_ = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2836ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_csv_files():\n",
    "    L = []\n",
    "    for file in os.listdir():\n",
    "        if file.endswith(\".csv\"):\n",
    "            L.append(file)\n",
    "    return L\n",
    "\n",
    "files = return_csv_files()\n",
    "\n",
    "class dict_(dict): \n",
    "    ### our own dictionary class we can shuffle the values easly\n",
    "    def __init__(self, kwargs):\n",
    "        super().__init__(kwargs)\n",
    "    def shuffle(self):\n",
    "        for keys, values in self.items():\n",
    "            if not isinstance(values, list):\n",
    "                self[keys] = list(values)\n",
    "            np.random.shuffle(self[keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f471ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class open_pd:\n",
    "    ### This dude returns dataframes using multithreading ### \n",
    "    ### To overcome performance issues  with bottleneck reduction ###\n",
    "    def __init__(self, max_workers = 3):\n",
    "        self.L = []\n",
    "        self.max_workers = max_workers\n",
    "    def append(self, file):\n",
    "        (self.L).append(pd.read_csv(file))\n",
    "    def return_csv_files(self, files):\n",
    "        self.__return_csv_files__(files)\n",
    "        return self.L\n",
    "    def __return_csv_files__(self, files):\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers = self.max_workers) as executor:\n",
    "            executor.map(self.append, files)\n",
    "    def reset(self):\n",
    "        self.L = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "014ccf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = return_csv_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "252ce090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader_(IterableDataset):\n",
    "    def __init__(self, list_csv:list, chunk_size:int = 3, lags = 512, **kwargs):\n",
    "        super(data_loader_).__init__(**kwargs)\n",
    "        \"\"\"\n",
    "        list_csv : list of csv file \n",
    "        chunk_size : chunk_size many csv file will open up into memory and converted into tensors\n",
    "        \"\"\"\n",
    "        self.csv_files = list_csv ### names of csv files\n",
    "        self.csv_index = [i for i, _ in enumerate(self.csv_files)] ### enumeration of related time series \n",
    "        self.num_csv = len(self.csv_index)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.lags = lags \n",
    "        ### -- technical stuff -- ###\n",
    "        \n",
    "        \n",
    "        ### --- indexing --- ###\n",
    "        \n",
    "    def __iter__(self):\n",
    "        ### shuffle the things before getting started ###\n",
    "        np.random.shuffle(self.csv_index)\n",
    "        for indice in range(int((self.num_csv/self.chunk_size))+1):\n",
    "            indices = self.csv_index[indice*self.chunk_size: self.chunk_size*(indice+1)]\n",
    "            __dict__ = {self.csv_index[i]:pd.read_csv(self.csv_files[i]) for i in indices}\n",
    "            __indices__ = dict_({keys:range(self.lags,len(values)-1) for keys, values in __dict__.items()})\n",
    "            __indices__.shuffle()\n",
    "            if len(indices) > 0:\n",
    "                for keys, values in __indices__.items():\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9644b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = data_loader_(files, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "666338ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([123, 106, 19, 235, 89, 71, 26, 232, 167, 21, 256, 234, 9, 152, 177], 698635)\n",
      "([105, 143, 83, 151, 7, 164, 120, 121, 208, 47, 204, 113, 257, 72, 24], 613016)\n",
      "([109, 209, 147, 84, 70, 136, 155, 236, 58, 240, 161, 68, 206, 269, 254], 751230)\n",
      "([175, 220, 179, 158, 75, 142, 34, 10, 246, 127, 145, 124, 253, 87, 129], 665773)\n",
      "([115, 102, 139, 38, 242, 61, 176, 140, 148, 12, 5, 261, 60, 170, 66], 638522)\n",
      "([11, 247, 81, 130, 174, 244, 77, 51, 52, 48, 156, 201, 187, 268, 44], 659997)\n",
      "([74, 30, 56, 188, 239, 23, 122, 153, 55, 94, 85, 226, 4, 116, 45], 693294)\n",
      "([231, 90, 25, 31, 88, 53, 213, 108, 91, 182, 243, 249, 29, 221, 191], 894002)\n",
      "([199, 98, 255, 178, 133, 73, 135, 110, 183, 80, 215, 198, 165, 101, 251], 782013)\n",
      "([97, 230, 228, 197, 62, 15, 132, 2, 159, 200, 225, 36, 252, 173, 186], 897581)\n",
      "([260, 112, 125, 128, 3, 258, 263, 64, 137, 222, 149, 265, 250, 169, 190], 669803)\n",
      "([131, 67, 86, 141, 192, 14, 39, 119, 114, 49, 227, 205, 28, 262, 172], 888368)\n",
      "([184, 223, 241, 229, 203, 27, 207, 134, 196, 154, 54, 160, 210, 181, 185], 684548)\n",
      "([78, 32, 100, 218, 107, 99, 126, 41, 238, 211, 216, 17, 163, 103, 248], 910574)\n",
      "([193, 35, 171, 217, 267, 37, 259, 168, 118, 8, 42, 157, 65, 146, 212], 823126)\n",
      "([194, 6, 92, 104, 224, 245, 162, 63, 40, 138, 202, 237, 264, 144, 59], 612894)\n",
      "([46, 13, 95, 57, 233, 33, 166, 93, 117, 43, 1, 195, 96, 111, 180], 710048)\n",
      "([18, 266, 0, 150, 69, 82, 76, 20, 219, 50, 79, 16, 214, 189, 22], 701161)\n"
     ]
    }
   ],
   "source": [
    "for i in loader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f7b55755",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series(np.random.randn(1000))\n",
    "b = pd.Series(np.random.randn(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "48628576",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = pd.concat([a,b], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "146b897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    #We may cythonize this part for speeding up the things -->\n",
    "    def __getindices__(self, K:list, L:list)->tuple:\n",
    "        assert len(K) == len(L)\n",
    "        K_ = []\n",
    "        L_ = []\n",
    "        N = len(K)\n",
    "        for i in range(N-length):\n",
    "            if L[i] == L[i+length]:\n",
    "                K_.append(i)\n",
    "                L_.append(L[i])\n",
    "        return K_, L_\n",
    "    def __getlength__(self, data:list, classes:list):\n",
    "        \n",
    "        length = []\n",
    "        classes_ = []\n",
    "        k = 0\n",
    "        for enum, data_  in enumerate(data):\n",
    "            for _ in data_.values:\n",
    "                length.append(k)\n",
    "                classes_.append(classes[enum])\n",
    "                k += 1\n",
    "        return length, classes_\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965ee01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
